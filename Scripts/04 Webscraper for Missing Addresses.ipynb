{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from time import sleep\n",
    "import webbrowser\n",
    "from os.path import expanduser\n",
    "from os import listdir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = expanduser(\"~\") + \"/chromedriver\"\n",
    "\n",
    "if 'chromedriver.exe' not in listdir(expanduser(\"~\")):\n",
    "    print('chomedriver.exe not found in the home directory! Refer to Selenium docs.')\n",
    "    sys.exit()\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=executable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#opportunities = pd.read_csv('opportunities.csv')\n",
    "#use this if restarting scraper\n",
    "opportunities = pd.read_csv('opportunities_scraped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opportunities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#opportunities['scraped_address']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opportunities[opportunities.scraped_address!='not found'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opportunities.head(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=\"/Users/sianlewis/Downloads/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export to csv\n",
    "opportunities.to_csv('opportunities_scraped.csv',index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape():\n",
    "    '''Use Seleneium to conduct a Google search for each Opppotunity Name\n",
    "       that does not have an address. If an address is returned, add the\n",
    "       address to the opprtunities data frame. Save final results to a csv.\n",
    "    '''\n",
    "    driver = webdriver.Chrome(\n",
    "        executable_path=\"/Users/sianlewis/Downloads/chromedriver\")\n",
    "    counter = 0\n",
    "    for i, row in opportunities[      \n",
    "            #(opportunities['scraped_address'].isnull() == True)].iterrows():\n",
    "            (opportunities['scraped_address'].isnull())].iterrows():\n",
    "        search_term = ''\n",
    "        url = ''\n",
    "        address = ''\n",
    "        search_term = row['Opportunity Name'].replace(' ', '+')+'+address'\n",
    "        url = 'https://www.google.com/search?\\&q='+search_term\n",
    "        driver.get(url)\n",
    "        search_bar = driver.find_element_by_id('lst-ib')\n",
    "        search_bar.clear()\n",
    "        search_bar.send_keys(search_term)\n",
    "        response = driver.page_source\n",
    "        html = BeautifulSoup(response, 'lxml')\n",
    "        span = html.find('span', {'class':'LrzXr'})\n",
    "        #print (i, search_term)\n",
    "        try:\n",
    "            address = span.text\n",
    "        except:\n",
    "            address = 'not found'\n",
    "        #print(address)\n",
    "        opportunities.loc[opportunities['Opportunity ID'] == row['Opportunity ID'],\n",
    "                          'scraped_address'] = address\n",
    "        counter += 1\n",
    "        if counter%20 == 0:\n",
    "            print(i, search_term)\n",
    "            print(address)\n",
    "            #sleep(10)\n",
    "\n",
    "print('start')\n",
    "keep_going = True\n",
    "while keep_going:\n",
    "    try:\n",
    "        scrape()\n",
    "        keep_going = False\n",
    "    except:\n",
    "        opportunities.to_csv('opportunities_scraped.csv',index_label=False)\n",
    "    \n",
    "opportunities.to_csv('opportunities_scraped.csv',index_label=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
